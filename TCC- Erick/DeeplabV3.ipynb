{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/autoceres/VisionCERES/blob/master/DeeplabV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LB62ukidlZG",
        "outputId": "45c2c146-a3e8-4e06-f8c6-369f3cd9f8eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_hv2gMZhr6_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from albumentations import HorizontalFlip, ChannelShuffle, CoarseDropout, Rotate, Equalize, RandomShadow, CenterCrop, RandomBrightnessContrast\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.layers import AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, Reshape, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50, ResNet101\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xjOji6LgTO7"
      },
      "outputs": [],
      "source": [
        "\"\"\" Diretório \"\"\"\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path, split = 0.15):\n",
        "    \"\"\" Carregamento das imagens e máscaras \"\"\"\n",
        "    X = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/TCC/dataset_tcc/images_final\", \"*.png\")))\n",
        "    Y = sorted(glob(os.path.join(path, \"/content/gdrive/MyDrive/TCC/dataset_tcc/masks_final\", \"*.png\")))\n",
        "\n",
        "    split_size = int(len(X) * split)\n",
        "\n",
        "    train_x, test_x = train_test_split(X, test_size = split_size, random_state = 42)\n",
        "    train_y, test_y = train_test_split(Y, test_size = split_size, random_state = 42)\n",
        "\n",
        "    return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "def augment_data(images, masks, save_path, augment = True):\n",
        "    H = 512\n",
        "    W = 512\n",
        "\n",
        "    for x, y in tqdm(zip(images, masks), total = len(images)):\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
        "\n",
        "        \"\"\" Data Augmentation \"\"\"\n",
        "        if augment == True:\n",
        "            aug = HorizontalFlip(p = 1.0) #Flip horizontal\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x1 = augmented[\"image\"]\n",
        "            y1 = augmented[\"mask\"]\n",
        "\n",
        "            x2 = cv2.cvtColor(x, cv2.COLOR_RGB2GRAY)\n",
        "            y2 = y\n",
        "\n",
        "            \"\"\"\n",
        "            aug = ChannelShuffle(p = 1) #Reorganiza aleatoriamente os canais da imagem RGB de entrada.\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x3 = augmented['image']\n",
        "            y3 = augmented['mask']\n",
        "            \"\"\"\n",
        "\n",
        "            aug = CoarseDropout(p = 1, min_holes = 3, max_holes = 10, max_height = 32, max_width = 32) #Dropa uma caixa de pixels 32x32 da imagem, pode simular possíveis falhas na linha de plantação\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x4 = augmented['image']\n",
        "            y4 = augmented['mask']\n",
        "            \n",
        "            \"\"\"\n",
        "            aug = Rotate(limit = 45, p = 1.0) #Rotaciona a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = Equalize(p = 1.0) #Equaliza a imagem de entrada\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x5 = augmented[\"image\"]\n",
        "            y5 = augmented[\"mask\"]\n",
        "\n",
        "            \"\"\"\n",
        "            aug = RandomShadow(shadow_roi = (0, 0.5, 1, 1), num_shadows_lower = 1, num_shadows_upper = 1, shadow_dimension = 4, always_apply = False, p=1) #Cria sombras nas imagens\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "            \"\"\"\n",
        "\n",
        "            aug = RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, brightness_by_max=True, always_apply=False, p=1.5) #Brilho aleatório\n",
        "            augmented = aug(image = x, mask = y)\n",
        "            x6 = augmented[\"image\"]\n",
        "            y6 = augmented[\"mask\"]\n",
        "\n",
        "            X = [x, x1, x2, x4, x5, x6]\n",
        "            Y = [y, y1, y2, y4, y5, y6]\n",
        "\n",
        "        else:\n",
        "            X = [x]\n",
        "            Y = [y]\n",
        "\n",
        "        index = 0\n",
        "        for i, m in zip(X, Y):\n",
        "            try:\n",
        "                aug = CenterCrop(H, W, p = 1.0)\n",
        "                augmented = aug(image = i, mask = m)\n",
        "                i = augmented[\"image\"]\n",
        "                m = augmented[\"mask\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                i = cv2.resize(i, (W, H))\n",
        "                m = cv2.resize(m, (W, H))\n",
        "\n",
        "            tmp_image_name = f\"{name}_{index}.png\"\n",
        "            tmp_mask_name = f\"{name}_{index}.png\"\n",
        "\n",
        "            image_path = os.path.join(save_path, \"image\", tmp_image_name)\n",
        "            mask_path = os.path.join(save_path, \"mask\", tmp_mask_name)\n",
        "\n",
        "            cv2.imwrite(image_path, i)\n",
        "            cv2.imwrite(mask_path, m)\n",
        "\n",
        "            index += 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "\n",
        "    \"\"\" Carregamento do Dataset \"\"\"\n",
        "    data_path = \"dataset\"\n",
        "    (train_x, train_y), (test_x, test_y) = load_data(data_path)\n",
        "\n",
        "    print(f\"Train:\\t {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Test:\\t {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    \"\"\" Criando os diretórios para o Data Augmentation \"\"\"\n",
        "    create_dir(\"new_data/train/image/\")\n",
        "    create_dir(\"new_data/train/mask/\")\n",
        "    create_dir(\"new_data/test/image/\")\n",
        "    create_dir(\"new_data/test/mask/\")\n",
        "\n",
        "    augment_data(train_x, train_y, \"new_data/train/\", augment = True)\n",
        "    augment_data(test_x, test_y, \"new_data/test/\", augment = False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqy4fcadhzYK",
        "outputId": "80100b2b-c104-4efa-f9c7-9f376bc11475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 8s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def SqueezeAndExcite(inputs, ratio = 8):\n",
        "    init = inputs\n",
        "    filters = init.shape[-1]\n",
        "    se_shape = (1, 1, filters)\n",
        "\n",
        "    se = GlobalAveragePooling2D()(init)\n",
        "    se = Reshape(se_shape)(se)\n",
        "    se = Dense(filters // ratio, activation = 'relu', kernel_initializer = 'he_normal', use_bias = False)(se)\n",
        "    se = Dense(filters, activation = 'sigmoid', kernel_initializer = 'he_normal', use_bias = False)(se)\n",
        "    x = init * se\n",
        "    return x\n",
        "\n",
        "def ASPP(inputs):\n",
        "    \"\"\" Image Pooling \"\"\"\n",
        "    shape = inputs.shape\n",
        "    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(inputs)\n",
        "    y1 = Conv2D(256, 1, padding = \"same\", use_bias = False)(y1)\n",
        "    y1 = BatchNormalization()(y1)\n",
        "    y1 = Activation(\"relu\")(y1)\n",
        "    y1 = UpSampling2D((shape[1], shape[2]), interpolation = \"bilinear\")(y1)\n",
        "\n",
        "    \"\"\" 1x1 conv \"\"\"\n",
        "    y2 = Conv2D(256, 1, padding = \"same\", use_bias = False)(inputs)\n",
        "    y2 = BatchNormalization()(y2)\n",
        "    y2 = Activation(\"relu\")(y2)\n",
        "\n",
        "    \"\"\" 3x3 conv rate = 6 \"\"\"\n",
        "    y3 = Conv2D(256, 3, padding = \"same\", use_bias = False, dilation_rate = 6)(inputs)\n",
        "    y3 = BatchNormalization()(y3)\n",
        "    y3 = Activation(\"relu\")(y3)\n",
        "\n",
        "    \"\"\" 3x3 conv rate = 12 \"\"\"\n",
        "    y4 = Conv2D(256, 3, padding = \"same\", use_bias = False, dilation_rate = 12)(inputs)\n",
        "    y4 = BatchNormalization()(y4)\n",
        "    y4 = Activation(\"relu\")(y4)\n",
        "\n",
        "    \"\"\" 3x3 conv rate = 18 \"\"\"\n",
        "    y5 = Conv2D(256, 3, padding = \"same\", use_bias = False, dilation_rate = 18)(inputs)\n",
        "    y5 = BatchNormalization()(y5)\n",
        "    y5 = Activation(\"relu\")(y5)\n",
        "\n",
        "    y = Concatenate()([y1, y2, y3, y4, y5])\n",
        "    y = Conv2D(256, 1, padding = \"same\", use_bias = False)(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Activation(\"relu\")(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "def deeplabv3_plus(shape):\n",
        "    \"\"\" Input \"\"\"\n",
        "    inputs = Input(shape)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    #encoder = ResNet50(weights = \"imagenet\", include_top = False, input_tensor = inputs)\n",
        "    encoder = ResNet101(weights = \"imagenet\", include_top = False, input_tensor = inputs)\n",
        "\n",
        "    image_features = encoder.get_layer(\"conv4_block6_out\").output\n",
        "    x_a = ASPP(image_features)\n",
        "    x_a = UpSampling2D((4, 4), interpolation = \"bilinear\")(x_a)\n",
        "\n",
        "    x_b = encoder.get_layer(\"conv2_block2_out\").output\n",
        "    x_b = Conv2D(filters = 48, kernel_size = 1, padding = 'same', use_bias = False)(x_b)\n",
        "    x_b = BatchNormalization()(x_b)\n",
        "    x_b = Activation('relu')(x_b)\n",
        "\n",
        "    x = Concatenate()([x_a, x_b])\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = Conv2D(filters = 256, kernel_size = 3, padding = 'same', use_bias = False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters = 256, kernel_size = 3, padding = 'same', use_bias = False)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = SqueezeAndExcite(x)\n",
        "\n",
        "    x = UpSampling2D((4, 4), interpolation = \"bilinear\")(x)\n",
        "    x = Conv2D(1, 1)(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = deeplabv3_plus((512, 512, 3))\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfQN_Jmvio13"
      },
      "outputs": [],
      "source": [
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def shuffling(x, y):\n",
        "    x, y = shuffle(x, y, random_state = 42)\n",
        "    return x, y\n",
        "\n",
        "def load_data(path):\n",
        "    x = sorted(glob(os.path.join(path, \"image\", \"*png\")))\n",
        "    y = sorted(glob(os.path.join(path, \"mask\", \"*png\")))\n",
        "    return x, y\n",
        "\n",
        "def read_image(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path):\n",
        "    path = path.decode()\n",
        "    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis = -1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y):\n",
        "    def _parse(x, y):\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y, batch = 2):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(10)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Hiperparâmetros \"\"\"\n",
        "    batch_size = 8\n",
        "    lr = 1e-4\n",
        "    num_epochs = 20\n",
        "    model_path = os.path.join(\"files\", \"model.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"data.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"new_data\"\n",
        "    train_path = os.path.join(dataset_path, \"train\")\n",
        "    valid_path = os.path.join(dataset_path, \"test\")\n",
        "\n",
        "    train_x, train_y = load_data(train_path)\n",
        "    train_x, train_y = shuffling(train_x, train_y)\n",
        "    valid_x, valid_y = load_data(valid_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch = batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch = batch_size)\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = deeplabv3_plus((H, W, 3))\n",
        "    model.compile(loss = dice_loss, optimizer = Adam(lr), metrics = [dice_coef, iou, Recall(), Precision()])\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose = 1, save_best_only = True),\n",
        "        ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 5, min_lr = 1e-7, verbose = 1),\n",
        "        CSVLogger(csv_path),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor = 'val_loss', patience = 20, restore_best_weights = False),\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs = num_epochs,\n",
        "        validation_data = valid_dataset,\n",
        "        callbacks = callbacks\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH2E_dS5TGRI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "def save_results(image, mask, y_pred, save_image_path):\n",
        "    ## i - m - yp - yp*i\n",
        "    line = np.ones((H, 10, 3)) * 128\n",
        "\n",
        "    mask = np.expand_dims(mask, axis = -1)  \n",
        "    mask = np.concatenate([mask, mask, mask], axis = -1) \n",
        "    mask = mask * 255\n",
        "\n",
        "    y_pred = np.expand_dims(y_pred, axis = -1) \n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis = -1) \n",
        "\n",
        "    masked_image = image * y_pred\n",
        "    y_pred = y_pred * 255\n",
        "\n",
        "    cat_images = np.concatenate([image, line, mask, line, y_pred, line, masked_image], axis = 1)\n",
        "    cv2.imwrite(save_image_path, cat_images)\n",
        "\n",
        "\n",
        "def save_results_pred(y_pred, save_image_path):\n",
        "    y_pred = np.expand_dims(y_pred, axis = -1) \n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis = -1) \n",
        "    y_pred = y_pred * 255\n",
        "\n",
        "    cv2.imwrite(save_image_path, y_pred)\n",
        "\n",
        "def save_results_masked(image, y_pred, save_image_path):\n",
        "    y_pred = np.expand_dims(y_pred, axis = -1) \n",
        "    y_pred = np.concatenate([y_pred, y_pred, y_pred], axis = -1) \n",
        "    masked_image = image * y_pred\n",
        "\n",
        "    cv2.imwrite(save_image_path, masked_image)\n",
        "    #cv2.imwrite(save_image_path, image)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    create_dir(\"results_pred\")\n",
        "    create_dir(\"results_masked\")\n",
        "    create_dir(\"results\")\n",
        "\n",
        "    \"\"\" Carregando o modelo \"\"\"\n",
        "    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "        model = tf.keras.models.load_model(\"files/model.h5\")\n",
        "\n",
        "    \"\"\" Carregando o Dataset \"\"\"\n",
        "    dataset_path = \"new_data\"\n",
        "    valid_path = os.path.join(dataset_path, \"test\")\n",
        "    test_x, test_y = load_data(valid_path)\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y)}\")\n",
        "\n",
        "    \"\"\" Avaliação e predição \"\"\"\n",
        "    SCORE = []\n",
        "    for x, y in tqdm(zip(test_x, test_y), total = len(test_x)):\n",
        "\n",
        "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        x = image/255.0\n",
        "        x = np.expand_dims(x, axis = 0)\n",
        "\n",
        "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
        "        #mask = cv2.imread(y)\n",
        "\n",
        "        #Predição\n",
        "        y_pred = model.predict(x)[0]\n",
        "        y_pred = np.squeeze(y_pred, axis = -1)\n",
        "        y_pred = y_pred > 0.5\n",
        "        y_pred = y_pred.astype(np.int32)\n",
        "\n",
        "        #Salvando a predição \n",
        "        save_image_path = f\"results/{name}.png\"\n",
        "        save_results(image, mask, y_pred, save_image_path)\n",
        "        save_image_path_pred = f\"results_pred/{name}.png\"\n",
        "        save_image_path_msk = f\"results_masked/{name}.png\"\n",
        "        save_results_pred(y_pred, save_image_path_pred)\n",
        "        save_results_masked(image, y_pred, save_image_path_msk)\n",
        "\n",
        "        mask = mask.flatten()\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "        #Calcula os valores das métricas\n",
        "        acc_value = accuracy_score(mask, y_pred)\n",
        "        f1_value = f1_score(mask, y_pred, labels=[0, 1], average = \"micro\")\n",
        "        jac_value = jaccard_score(mask, y_pred, labels=[0, 1], average = \"micro\")\n",
        "        recall_value = recall_score(mask, y_pred, labels=[0, 1], average = \"micro\")\n",
        "        precision_value = precision_score(mask, y_pred, labels=[0, 1], average = \"micro\")\n",
        "        SCORE.append([name, acc_value, f1_value, jac_value, recall_value, precision_value])\n",
        "\n",
        "    #Valores das métricas\n",
        "    score = [s[1:]for s in SCORE]\n",
        "    score = np.mean(score, axis = 0)\n",
        "    print(f\"Accuracy: {score[0]:0.5f}\")\n",
        "    print(f\"F1: {score[1]:0.5f}\")\n",
        "    print(f\"Jaccard: {score[2]:0.5f}\")\n",
        "    print(f\"Recall: {score[3]:0.5f}\")\n",
        "    print(f\"Precision: {score[4]:0.5f}\")\n",
        "\n",
        "    df = pd.DataFrame(SCORE, columns = [\"Image\", \"Accuracy\", \"F1\", \"Jaccard\", \"Recall\", \"Precision\"])\n",
        "    df.to_csv(\"files/score.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwFfzDL20t06"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/results_pred.zip /content/results_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvXz4pn95S8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ae513fb-38d0-454c-8087-34cc2267dc1a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0fcc2448-9926-4441-a8c8-49a92b074182\", \"files.zip\", 197086042)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/files.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
